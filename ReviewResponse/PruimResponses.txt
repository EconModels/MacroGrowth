
Matt,

below are some thoughts on the items you listed.  The pagination has changed in 
overleaf, so I was estimating part of the text that led to these questions.

---------------


- Reviewer 2 said: "Page 21: Why don’t capital and labour output elasticities
  mean the same thing across different modelling approaches – or across
  different nesting structures? After all, their definition is unchanged?”

  I'm not sure how much of answer you want to give, but we can
  
    * piont out that equation 8 is not the same as equations 6 and 7 because the 
    APF is not symmetric in its three inputs.

    * refer to Figure 6, which shows how different output elasticities can be when 
    fitting different models to the same data.
 
  For the latter, I guess this doesn't distinguish between the models disaggreeing 
  about what the output elasticities mean and what the values are, but if they mean 
  precisely the same thing, then that thing is highly sensitive to the model used.

  I'll have to think about whether there is a better, more direct way to answer
  this.

-------------------------

- Reviewer 2 said: “p. 3: In the econometrics I'm familiar with, it is common
  to include heteroscedasticity, serial correlation and normality tests of the
  residuals, along with tests for multicollinearity, RESET and so on. But Step
  3 here (and the subsequent empirical analysis) only mentions goodness of fit
  and information criteria. Is that because other tests are inappropriate for
  the non-linear techniques used in this paper, or simply because they haven't
  been considered??”

I guess all of that stuff could be covered by "etc".  We can certainly list 
additional items there if we like.  

Note 1: Tests for heteroscedasticity, serial 
correletion, and the like are only required if the inference methods rely on the 
corresponding assumptions (homoskedasticity; additive, normal, independent errors, etc.)
and are not robust against departures from these assumptions.

Note 2: It isn't clear to me that some of this isn't really part of Step 4 rather than 
Step 3 since it is more about evaluating models than about fitting them.

You might mention that we did in fact (and still have in the supplemental material?)
do some of these other checks.


==> Please provide a 2-3 sentence response to this question.  Could we include
the reviewer’s suggestions in our list?  Or are they not applicable for some
reason? (I assigned this to you previously.)

---------------------

- Reviewer 2 said: "Page 19:  What does over-interpretation mean in this
  context?”

==> I would say “over-interpretation means reaching conclusions that are
unjustified when the full scope of results is considered, including,
especially, the precision with which estimates of model parameters are known.”
Would you agree?  (I assigned this to you previously.)

That particular issue, while perhaps also an example of overinterpretation, isn't 
really the issue on page 19.  

Here is an anology for page 19:  Let's suppose three golfers go golfing on the
same course.  One shoots 92, one 85, the third 82.  You are asked to determine
who the best golfer is based on this.  Since there is variability from round to
round, you know that you can't be certain.  But the situation is worse for you,
because the golfers didn't play by the same rules.

  * Golfer 1 played by normal rules of golf.
  * Golfer 2 hit two drives from each tee box and played the better one.
  * Golfer 3 allowed himself one mulligan (not necessarily the drive) on each hole

Now it become more difficult to compare the golfers.  But it is clear that a
straight-up comparison of the scores is not meaningful.  Assuming the third
golfer could score 82 playing by standard rules would be overinterpreting his
performance.

Fitting multiple models and selecting the best one is somewhat like being
allowed additional shots in golf.  The resuting score (p-value, confidence
interval, etc.) will look better, but it won't mean as much.

A possible fix might be to move overinterpretation to later in the paragraph.
Perhaps something like this:

Often, after considering (or \emph{potentially} considering) multiple modeling
approaches, researchers select a ``best'' one and the resulting modeling
approach is analyzed and interpreted as if it were the only one considered.
When the ``researcher degrees of freedom'' \citep{Simmons:2011} are undisclosed
\deletedtext{and} \insertedtext{or} unaccounted for, the strength of the
evidence provided in the data is necessarily inflated.
\insertedtext{This can lead to an over-interpretation of the results
unless corrections are made in a way that is similar 
to corrections for the ``multiple comparisons'' problem.}%
This is not the place to elaborate on or advocate for particular solutions to
multi-model inference,% 
\footnote{Some of the challenges of multi-model inference and one approach to 
handling them are provided in \citet{Burnham:2002}.}


-----------------

- Reviewer 2 said: "Page 19:  To avoid overfitting, one would normally use an
  information criteria such as AIC. Why isn’t that recommended here?”

Using AIC or some other criterion for selecting a "winning" model is helpful for
comparing models with different degrees of freedom, but it doesn't address the 
issues of multiple models and the likelihood that the best of several models -- 
regardless of the definition of best -- is likely to be worse than it appears in 
isolation.


